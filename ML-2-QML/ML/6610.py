"""Hybrid classical regression module with data augmentation and cross‑validation.

The module defines:
    * `generate_superposition_data` – generates synthetic regression data.
    * `augment_with_quantum` – mixes a fraction of quantum‑encoded samples into a classical batch.
    * `RegressionDataset` – torch Dataset yielding features and target.
    * `QuantumRegression__gen456` – a neural network that can optionally concatenate a quantum‑encoded vector and outputs regression and classification heads.
    * `evaluate_cross_validation` – helper that runs k‑fold CV on a DataLoader.
"""

from __future__ import annotations

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import KFold
from typing import Tuple, List

def generate_superposition_data(num_features: int, samples: int) -> Tuple[np.ndarray, np.ndarray]:
    x = np.random.uniform(-1.0, 1.0, size=(samples, num_features)).astype(np.float32)
    angles = x.sum(axis=1)
    y = np.sin(angles) + 0.1 * np.cos(2 * angles)
    return x, y.astype(np.float32)

def augment_with_quantum(x: np.ndarray, y: np.ndarray, mix_ratio: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:
    """Mix a fraction of quantum‑encoded samples into the classical batch.

    The quantum state is generated by the same superposition generator but
    flattened to a real vector via the probability amplitudes of the computational
    basis states.  Only the real part is kept for simplicity.
    """
    samples = x.shape[0]
    quantum_states, _ = generate_superposition_data(num_features=x.shape[1], samples=samples)
    mask = np.random.rand(samples) < mix_ratio
    x[mask] = quantum_states[mask].real
    return x, y

class RegressionDataset(Dataset):
    def __init__(self, samples: int, num_features: int):
        self.features, self.labels = generate_superposition_data(num_features, samples)

    def __len__(self) -> int:  # type: ignore[override]
        return len(self.features)

    def __getitem__(self, index: int):  # type: ignore[override]
        return {
            "states": torch.tensor(self.features[index], dtype=torch.float32),
            "target": torch.tensor(self.labels[index], dtype=torch.float32),
        }

class QuantumRegression__gen456(nn.Module):
    def __init__(self, num_features: int, hidden_units: int = 32, use_quantum: bool = False):
        super().__init__()
        self.use_quantum = use_quantum
        self.feature_extractor = nn.Sequential(
            nn.Linear(num_features, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, hidden_units // 2),
            nn.ReLU(),
        )
        self.regression_head = nn.Linear(hidden_units // 2, 1)
        self.classification_head = nn.Linear(hidden_units // 2, 2)  # binary

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        feats = self.feature_extractor(x)
        reg = self.regression_head(feats).squeeze(-1)
        cls = self.classification_head(feats)
        return {"regression": reg, "classification": cls}

    def evaluate_cross_validation(
        self,
        loader: DataLoader,
        k_folds: int = 5,
        device: torch.device = torch.device("cpu"),
    ) -> Tuple[float, float]:
        """Return mean MSE and accuracy over k‑fold cross‑validation."""
        dataset = loader.dataset
        indices = np.arange(len(dataset))
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        mse_scores: List[float] = []
        acc_scores: List[float] = []

        for train_idx, val_idx in kf.split(indices):
            train_subset = Subset(dataset, train_idx)
            val_subset = Subset(dataset, val_idx)
            train_loader = DataLoader(train_subset, batch_size=loader.batch_size, shuffle=True)
            val_loader = DataLoader(val_subset, batch_size=loader.batch_size, shuffle=False)

            self.train()
            optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
            criterion = nn.MSELoss()

            for _ in range(10):  # few epochs per fold
                for batch in train_loader:
                    optimizer.zero_grad()
                    outputs = self(batch["states"].to(device))
                    loss = criterion(outputs["regression"], batch["target"].to(device))
                    loss.backward()
                    optimizer.step()

            self.eval()
            mse = 0.0
            correct = 0
            total = 0
            with torch.no_grad():
                for batch in val_loader:
                    outputs = self(batch["states"].to(device))
                    mse += criterion(outputs["regression"], batch["target"].to(device)).item() * batch["states"].size(0)
                    preds = outputs["classification"].argmax(dim=1)
                    correct += (preds == (batch["target"].abs() > 0.5).long()).sum().item()
                    total += batch["states"].size(0)

            mse_scores.append(mse / total)
            acc_scores.append(correct / total)

        return float(np.mean(mse_scores)), float(np.mean(acc_scores))

__all__ = ["QuantumRegression__gen456", "RegressionDataset", "generate_superposition_data"]
