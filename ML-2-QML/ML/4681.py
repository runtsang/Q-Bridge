"""Combined classical convolutional filter, classifier and regression in a single module.

This class unifies the classical filter from Conv.py, the MLP classifier from
QuantumClassifierModel.py and the regression network from QuantumRegression.py.
The architecture is fully trainable in PyTorch and exposes the same public API for
both classification and regression tasks.  A learnable threshold parameter is
introduced to emulate the quantum encoding decision, allowing smooth gradient flow.
"""

from __future__ import annotations

import torch
from torch import nn
from torch.utils.data import Dataset
import numpy as np

class ConvGen127(nn.Module):
    """
    A hybrid classical network that can operate in classification or regression mode.
    Parameters
    ----------
    conv_kernel_size : int, default 2
        Size of the 2‑D convolution kernel.
    conv_threshold : float, default 0.0
        Initial threshold for the sigmoid non‑linearity.
    num_features : int, default 10
        Number of input features for the downstream MLP.
    depth : int, default 3
        Depth of the MLP (number of hidden layers).
    task : str, {"classification", "regression"}, default "classification"
        Which downstream task the network should perform.
    """

    def __init__(
        self,
        conv_kernel_size: int = 2,
        conv_threshold: float = 0.0,
        num_features: int = 10,
        depth: int = 3,
        task: str = "classification",
    ) -> None:
        super().__init__()
        self.task = task
        self.conv = nn.Conv2d(1, 1, kernel_size=conv_kernel_size, bias=True)
        self.threshold = nn.Parameter(torch.tensor(conv_threshold, dtype=torch.float32))
        # Build the downstream MLP
        in_dim = conv_kernel_size * conv_kernel_size
        layers = [nn.Flatten()]
        for _ in range(depth):
            layers.extend([nn.Linear(in_dim, num_features), nn.ReLU()])
            in_dim = num_features
        if task == "classification":
            layers.append(nn.Linear(in_dim, 2))
        else:
            layers.append(nn.Linear(in_dim, 1))
        self.mlp = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass that applies the convolutional filter and then the MLP.

        Parameters
        ----------
        x : torch.Tensor
            Input tensor of shape (B, 1, H, W) where H=W=conv_kernel_size.

        Returns
        -------
        torch.Tensor
            Logits for classification or a regression value.
        """
        conv_out = self.conv(x)
        # Apply a sigmoid with a learnable threshold
        logits = torch.sigmoid(conv_out - self.threshold)
        # Mean over spatial dimensions to get a scalar per sample
        pooled = logits.mean(dim=(2, 3))
        return self.mlp(pooled)

# --------------------------------------------------------------------------- #
# Dataset utilities
# --------------------------------------------------------------------------- #

def generate_superposition_data(num_features: int, samples: int) -> tuple[np.ndarray, np.ndarray]:
    """
    Sample states of the form cos(theta)|0…0> + e^{i phi} sin(theta)|1…1>.
    The target is a non‑linear combination of the angles.

    Parameters
    ----------
    num_features : int
        Dimensionality of the feature vector.
    samples : int
        Number of samples to generate.

    Returns
    -------
    tuple[np.ndarray, np.ndarray]
        Features and labels.
    """
    # Random angles
    thetas = 2 * np.pi * np.random.rand(samples)
    phis = 2 * np.pi * np.random.rand(samples)
    # Simple feature: sum of angles
    features = np.vstack([np.cos(thetas), np.sin(phis)]).T.astype(np.float32)
    labels = np.sin(2 * thetas) * np.cos(phis)
    return features, labels.astype(np.float32)

class SuperpositionDataset(Dataset):
    """
    PyTorch dataset wrapping features and labels generated by ``generate_superposition_data``.
    """

    def __init__(self, samples: int, num_features: int):
        self.features, self.labels = generate_superposition_data(num_features, samples)

    def __len__(self) -> int:  # type: ignore[override]
        return len(self.features)

    def __getitem__(self, idx: int):  # type: ignore[override]
        return {
            "states": torch.tensor(self.features[idx], dtype=torch.float32),
            "target": torch.tensor(self.labels[idx], dtype=torch.float32),
        }

__all__ = ["ConvGen127", "SuperpositionDataset", "generate_superposition_data"]
